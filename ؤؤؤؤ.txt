Slide Title: Chosen Data Mining & Machine Learning Algorithms
Why Use Machine Learning?
Our mobile application relies on machine learning to recognize sign language gestures,
translate them into text, and enable two-way communication. We selected the following algorithms based on accuracy,
real-time performance, and suitability for mobile environments.


ðŸ”¹ 1. MediaPipe Hands (Google AI) â€“ Hand Tracking
ðŸ“Œ Algorithm Type:

Computer Vision (Deep Learning-Based Hand Landmark Detection)
ðŸ“Œ Why?

Detects hand positions and finger movements in real-time.
Extracts 21 key points per hand, providing accurate gesture input.
Highly optimized for mobile devices (lightweight and fast).

ðŸ”¹ 2. SPOTER (Transformer Model) â€“ Sign Language Recognition
ðŸ“Œ Algorithm Type:

Transformer-Based Deep Learning Model
ðŸ“Œ Why?

Unlike traditional CNNs or RNNs, Transformers capture complex sign language gestures more effectively.
Works with sequences of hand movements to detect full words and phrases instead of single gestures.
Provides state-of-the-art accuracy for real-time sign language translation.
ðŸ”¹ 3. Google Speech-to-Text API â€“ Speech Recognition
ðŸ“Œ Algorithm Type:

Deep Learning (Recurrent Neural Networks & Transformers)
ðŸ“Œ Why?

Converts spoken language into text using advanced AI models trained on massive datasets.
Handles various accents, languages, and background noise efficiently.
ðŸ”¹ 4. Google Text-to-Speech API â€“ Speech Synthesis
ðŸ“Œ Algorithm Type:

Neural Text-to-Speech (TTS) Model
ðŸ“Œ Why?

Uses WaveNet (Deep Neural Network) to generate natural-sounding speech.
Converts text into speech, making communication easier for sign language users.